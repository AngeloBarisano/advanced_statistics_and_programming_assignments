---
title: "assignment_1_angelo_barisano_final"
output: html_document
date: '2022-09-13'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r import data & set wkdir}
# clear environment
rm(list=ls())

# import the necessary libraries
library("tidyverse")
library("stargazer")
library("tidyverse")
library("reshape")
library("Hmisc")
library("ggplot2")
library("dplyr")
library("moments")
library ("lm.beta")
library("fastDummies")
library("gtsummary")
library("stringr")


setwd("/home/angelo/Documents/Uni/Courses/Advanced Statistics and programming/Assignments/assignment1")

df <- read.csv("Data/train.csv", header= TRUE, sep= ",")

```

### Setting up a small function to fix the stargazer output
``` {r fix stargazer}
# credit: https://stackoverflow.com/questions/63959113/how-can-we-display-f-statistic-degrees-of-freedom-in-two-lines


show_F_in_two_lines <- function(stargazer) {
  # `Stringr` works better than base's regex 
  require(stringr)

  # If you remove `capture.output()`, not only the modified LaTeX code 
  # but also the original code would show up
  stargazer <- stargazer |>
    capture.output()

  # Reuse the index in which F-statistics are displayed
  position_F <- str_which(stargazer, "F Statistic")

  # Extract only F-statistics
  Fs <- stargazer[position_F] |>
    str_replace_all("\\(.*?\\)", "")

  # Extract only df values and make a new line for them
  dfs <- stargazer[position_F] |>
    str_extract_all("\\(.*?\\)") |>
    unlist() |>
    (
      \(dfs)
      paste0(" & ", dfs, collapse = "")
    )() |>
    paste0(" \\\\")

  # Reuse table elements that are specified
  # after the index of F-statistics
  after_Fs <- stargazer[-seq_len(position_F)]

  c(
    stargazer[seq_len(position_F - 1)],
    Fs,
    dfs,
    after_Fs
  ) |>
    cat(sep = "\n")
}
```



# Inspect missing values in the data before changing data types 
#### Eg. string conversion can lead to Null values to be interpreted as strings


```{r missing values inspection}
# display missing values by column
colSums(is.na(df))

# select all columns which have missing data 
which(colSums(is.na(df))>0)
```

```{r prep data}
df_ <- df[, c(
  'Id',
  'SalePrice',
  'MoSold',
  'YrSold',
  'YearRemodAdd',
  'LotArea',
  'GrLivArea',
  'TotalBsmtSF',
  'BldgType',
  'MSZoning',
  'Neighborhood',
  'OverallQual',
  'OverallCond'
)]

# create the interesting varaibles
df_$tot_liv_area <- df_$GrLivArea + df_$TotalBsmtSF

# time since remodeling at year of sale in years
df_$y_since_rem <- df_$YrSold - df_$YearRemodAdd



# adjust for family homes
# the distinction made here is simply that:
# stand alone house vs multiple houses together
match_df = data.frame(
  old = c("1Fam", "2fmCon", "Duplex",  "Twnhs", "TwnhsE"),
  new = c(
    "Single Family Home",
    "Multi-Unit Homes",
    "Multi-Unit Homes",
    "Multi-Unit Homes",
    "Multi-Unit Homes"
  )
)

df_ <-
  df_ %>% mutate(Building_type = match_df$new[match(BldgType, match_df$old)])



# group zoning
#' I have decided to leave the groups as small groups sizes are only a problem for anova
#' https://stats.stackexchange.com/questions/219071/sample-size-of-the-levels-of-a-categorical-variables -- CITE!
#' https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0229345
#' Additionally: the plot shows that even if these groups are small, they do not scatter that much
match_df = data.frame(
  old = c("C (all)", "FV", "RH", "RL", "RM"),
  new = c(
    "Commercial",
    "Floating Village",
    "High Density",
    "Low Density",
    "Moderate Density"
  )
)

df_ <-
  df_ %>% mutate(MSZoning = match_df$new[match(MSZoning, match_df$old)])

#' however, further analysis will show that these two groups are quite comparable; so for the purpose of this analysis they are merged as well
#' Commercial and FV are too different to even be considered merged
match_df = data.frame(
  old = c("Commercial", "Floating Village", "High Density", "Low Density", "Moderate Density"),
  new = c(
    "Commercial",
    "Floating Village",
    "Moderate2High Density",
    "Low Density",
    "Moderate2High Density"
  )
)

df_ <-
  df_ %>% mutate(MSZoning_gr = match_df$new[match(MSZoning, match_df$old)])


```



```{r inspect dypes of the variables at hand}
# adjust the scale of saleprice for easier handling
df_$SalePrice <- df_$SalePrice / 1000

# to make it clear that only certain operations can be performed on the data
# ID should be a character as string/factor operations are resptrictive by design
# i.e. no mathematical operations
df_$Id <- as.character(df_$Id)

# The month sold shoul also be considered a factor for the same reason before
df_$MoSold <- as.factor(df_$MoSold)
df_$YrSold <- as.factor(df_$YrSold)

# these are just standard variables to be converted to factors
df_$Neighborhood <- as.factor(df_$Neighborhood)
df_$BldgType <- as.factor(df_$BldgType)
df_$MSZoning <- as.factor(df_$MSZoning)

# OverallQual can be interpreted as both categorical and numeric; It wil be further elaobrated upon
df_$OverallQual_cat <- as.factor(df_$OverallQual)


# drop some columns which are now no logner needed
df_ = df_[, !(names(df_) %in% c("YearRemodAdd", "TotalBsmtSF", "GrLivArea"))]

str(df_)

```




```{r inspect missing values a last time}
# display missing values by column
colSums(is.na(df_))


# select all columns which have missing data 
which(colSums(is.na(df_))>0)

```




```{r stargazer stats}

df_temp <- df_ %>%
  select(-one_of(c('Id', "MoSold", "YrSold", "BldgType")))
print(skewness(df_temp$SalePrice))

stargazer(
  df_temp,
  type = 'text',
  omit.summary.stat = c("N"),
  summary.stat = c("mean", "sd", "min", "p25", "median", "p75", "max")
  , title="Descriptive Statistics of Numeric Indepdenent and Dependent Varaible"
  # , initial.zero = F
  # ,single.row=TRUE)
)

stargazer(
  df_temp,
  # type = 'text',
  omit.summary.stat = c("N"),
  summary.stat = c("mean", "sd", "min", "p25", "median", "p75", "max")
  , title="Descriptive Statistics of Numeric Indepdenent and Dependent Varaible"
  # , initial.zero = F
  # ,single.row=TRUE)
)

```



# Plots

```{r First Hypothesis Plot}
# livingSpace by SalePrice
pdf(file="h1.pdf")
scatter <- ggplot(df_, aes(tot_liv_area, SalePrice))
scatter + geom_point() + geom_smooth(method = "lm", color = "Red") + labs(x = 'Living space in square feet', y = "Sale Price in 1000s")+ xlim(0, 7000) + ggtitle("Your Title Here")
dev.off()

#' Add Descritption: Two outliers left out for representative reasons 

# smooth the plot
scatter <- ggplot(df_, aes(tot_liv_area, SalePrice))
scatter + geom_point() + geom_smooth() + labs(x = 'Living space in square feet', y = "Sale Price in 1000s")+ xlim(0, 7000) 

#' Hypothesis 1: Total Living Space (IV) has a positive effect (potitive association with) on Sales Price (DV)

```

```{r inspect whether MSZones can be merged to ease the analysis and increase readablity}


# Only inspect high density & Medium density to see if they ar compatible 
scatter <-
  ggplot(subset(df_, MSZoning == "High Density" |MSZoning == "Moderate Density"  ), aes(tot_liv_area, SalePrice, colour = MSZoning))

scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "MSZoning"), alpha = 0.1) + labs(x = 'Living space in square feet', y = "Sale Price in 1000s", color = 'MSZoning') 
#' interestingly: Moderate and High density appear to be quite comparable


bar <- ggplot(df_, aes(MSZoning, SalePrice))

bar + stat_summary(
  fun = mean,
  geom = "bar",
  fill = 'white',
  colour = "Black"
) + stat_summary(fun.data = mean_cl_normal, geom = "pointrange") + labs(x = 'Zoning', y = 'Sale Price')

#' Additionally, High and Moderate density are also in this case quite comparable in all their statistics
#' As such, one can assume that these two groups might be comparable and can be merged
#' However, there might be other reasons underlying this issue; such as the small sample size of High density 
```


```{r Second Hypothesis Plot}
# now drill down
pdf(file="h2.1.pdf")
scatter <-
  ggplot(df_, aes(tot_liv_area, SalePrice, colour = MSZoning))
scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "MSZoning"), alpha = 0.1) + labs(x = 'Living space in square feet', y = "Sale Price in 1000s", color = 'MSZoning') + xlim(0, 7000)
dev.off()



# Now obseve the groups we identified before
pdf(file="h2.2.pdf")
scatter <-
  ggplot(df_, aes(tot_liv_area, SalePrice, colour = MSZoning_gr))
scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "MSZoning_gr"), alpha = 0.1) + labs(x = 'Living space in square feet', y = "Sale Price in 1000s", color = 'MSZoning_gr') + xlim(0, 7000)
dev.off()
#' While not incredibly strong we would expect the effect of Lining Space on Sale Price to be stronger for Low density 
#' areas than for High or Medium; in this case Moderate2High density



#' Hypothesis 2: The effect of Total Living Space on Sales Price is stronger (association is stronger) for Low Density zones than for Moderate OR High Density areas; A second part of this analysis invols Moderate2High 

```



```{r plots, echo=FALSE}
# Subanalysis; Neighborhoods & Clusters

scatter <- ggplot(df_, aes(tot_liv_area, SalePrice))
scatter + geom_point() + geom_smooth(method = "lm", color = "Red") + labs(x = 'Living space in square feet', y = "Sale Price in 1000s")


scatter <- ggplot(df_, aes(tot_liv_area, SalePrice, colour = Neighborhood))
scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "Neighborhood"), alpha = 0.1) + labs(x = 'Living space in square feet', y = "Sale Price in 1000s", color = 'Neighborhood')


scatter <- ggplot(df_, aes(tot_liv_area, SalePrice))
scatter + geom_point() + geom_smooth(method = "lm", color = "Red") + labs(x = 'Living space in square feet', y = "Sale Price in 1000s") + xlim(0, 7000)


scatter <- ggplot(df_, aes(tot_liv_area, SalePrice, colour = Neighborhood))
scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "Neighborhood"), alpha = 0.1) + labs(x = 'Living space in square feet', y = "Sale Price in 1000s", color = 'Neighborhood') + xlim(0, 7000)

df_subset <-
  subset(
    df_,
    Neighborhood == "NoRidge" |
      Neighborhood == "OldTown" |
      Neighborhood == "CollgCr" |
      Neighborhood == "NridgHt" 
  )

scatter <- ggplot(df_subset, aes(tot_liv_area, SalePrice))
scatter + geom_point() + geom_smooth(method = "lm", color = "Red") + labs(x = 'Living space in square feet', y = "Sale Price in 1000s") + xlim(0, 7000)


scatter <- ggplot(df_subset, aes(tot_liv_area, SalePrice, colour = Neighborhood))
scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "Neighborhood"), alpha = 0.1) + labs(x = 'Living space in square feet', y = "Sale Price in 1000s", color = 'Neighborhood') + xlim(0, 7000)

#' generally, as can be seen in these polots, the associations tend to cluster under certain neighborhoods and seem to be quite linear; However, this is less clear due to the amount of Neighborhoods and visually, this variable is not easy to be analysed
#' Additionally, it is to be expected that this variable would have a confounder possibly explaining a lot of the variance in the sales price; Crime by neighborhood
#' this necessitates another study to this end with other data 
```


```{r Control for quality and age of the house in general; hypothesis 3, echo=FALSE}
#' the association betwen the state of the house (in terms of qualtiy) and the years since it was renovated/remodeled 
#' is clear considering the first plot 
scatter <- ggplot(df_, aes(y_since_rem, SalePrice))
scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "y_since_rem"), alpha = 0.4) + labs(x = 'Time Since last Remodeling at point of sale', y = "Sale Price in 1000s")
#' Increasing cears since the house was remodeled (at time of sale of house) has a negative association with Sale price; as is to be expected


#' subseqeutnly, the connection between quality of the house, which represents its features and the quality of the house as 
#' a whole then might be impacted by the years since remodeling
#' 
#' The interpretation would follow as: For eg Qualitygroup2 compared to quality group1; for an increase in time since remodeling, the association of Qualitygroup2 with Sale price will decrease by amount X
#' 
#' What is interesting with this question is that it opens up: WHEN does the quality get reduced
#' Furthermore, when treating qualtiy as a numeric variable, a similar effect can be observed, as will be demonstrated later 
#' 
#' finally, it is notable that ALL categories are more negative than the first category in this moderation term. This was to be expected; which suggests that overall higher quality corresponds with "more recent or more modern houses"


ggplot(df_, aes(x = OverallQual_cat, y = SalePrice)) +
  geom_boxplot()+  xlab("Quality of the property") + ylab("Sale Price in 1000s") + labs(size = "Years Since Remodeling")


ggplot(df_, aes(x = OverallQual, y = SalePrice)) +
  geom_point(aes(size = y_since_rem))+  xlab("Quality of the property") + ylab("Sale Price in 1000s") + theme_set(theme_bw()+ theme(legend.position = "bottom")) + labs(size = "Years Since Remodeling")


# this is the hypothesis plot 


pdf(file="h3.pdf")
scatter <- ggplot(df_, aes(y_since_rem, SalePrice))
scatter + geom_point() + geom_smooth(method = "lm", aes(fill = "y_since_rem"), alpha = 0.4) + labs(x = 'Time Since last Remodeling at point of sale', y = "Sale Price in 1000s")
dev.off()


#' This shows that this variable is a great control for the quality of the house and its condition 
 
# As such: Years since remodeling have a negative impact on sales price 
```








```{r hypothesis 3, echo=FALSE}
# Zone on Building type

# the interesting question here is: are stand alone family homes more valuable? than multi-unit homes 

df_$MSZoning
table(df_$BldgType)


bar <- ggplot(df_, aes(BldgType, SalePrice))
bar + stat_summary(
  fun = mean,
  geom = "bar",
  fill = 'white',
  colour = "Black"
) + stat_summary(fun.data = mean_cl_normal, geom = "pointrange") + facet_wrap(~ MSZoning) + labs(x = 'BldgType', y = 'Sale Price') 








```








































